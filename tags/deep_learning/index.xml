<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep_learning on Varun</title>
    <link>https://vkkhare.github.io/tags/deep_learning/</link>
    <description>Recent content in deep_learning on Varun</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jul 2019 10:02:51 +0530</lastBuildDate>
    
	    <atom:link href="https://vkkhare.github.io/tags/deep_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Zero Shot Object Detection</title>
      <link>https://vkkhare.github.io/project/zero-shot-object-detection/</link>
      <pubDate>Thu, 11 Jul 2019 10:02:51 +0530</pubDate>
      
      <guid>https://vkkhare.github.io/project/zero-shot-object-detection/</guid>
      <description>&lt;p&gt;Zero shot detection greatly exacerbates the the problem of zero shot recognition. Not only training the classifiers pose a problem, we have to make the region proposal network learn to detect objects from unseen categories. These during training typically lie in the background.&lt;/p&gt;

&lt;p&gt;Our novelty lies in exploiting the visual consistency of the attributes via learning to detect attributes first and then combining these detections by a combinator module into final object detections. We employ spatial transformer networks to propagate the gradients across the combinator and RPNs. This was necessary due to unavailability of external bounding box annotations for attributes. It made it impossible to train RPN and combinator independently from classifier unlike faster-RCNN style architectures.&lt;/p&gt;

&lt;p&gt;This is work in progress and we aim to publish our results soon.&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adversarial Corruption</title>
      <link>https://vkkhare.github.io/project/adversarial-corruption/</link>
      <pubDate>Wed, 11 Apr 2018 09:57:10 +0530</pubDate>
      
      <guid>https://vkkhare.github.io/project/adversarial-corruption/</guid>
      <description>&lt;p&gt;Neural Networks have been observed to be robust to even adversarial corruptions. However, only a handful of theoretical results exist which guarantee robustness of NNs. We have made an attempt to address this, using results and techniques from robust statistics.
To measure the robustness of an algorithm, we want to derive its breakdown point which is the largest number of adversarially corrupted points an algorithm can handle and still guarantee recovery. The presence of breakdown point results for simpler learners like linear regression was a primary motivation to pursue this project using robust statistics.&lt;/p&gt;

&lt;p&gt;To simplify the problem, we consider the case of regression only single hidden layer neural networks with ReLU activation on each node and an output node with no activation with squared loss function. We analysed two different training procedures and how it can change the training trajectory to affect the final convergence. One apprach utilised the property of &lt;strong&gt;ReLU&lt;/strong&gt; of dividing the input space into 2 halves (active and unactive). Another approach split the problem as a difference of convex functions and we used alternating optimization to train it. This training procedure was well behaved in terms of convergence and theoretically sound.&lt;/p&gt;

&lt;p&gt;For complete details about these approaches please look in the attached pdf.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
